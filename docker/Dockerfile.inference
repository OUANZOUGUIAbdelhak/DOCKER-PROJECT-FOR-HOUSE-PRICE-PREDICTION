# ============================================================================
# INFERENCE DOCKERFILE
# ============================================================================
# Purpose: Build lightweight Docker image for model inference (predictions)
#
# Key Differences from Training Dockerfile:
# - Smaller image (no Jupyter, matplotlib, seaborn)
# - Faster startup (fewer dependencies)
# - Production-optimized (minimal attack surface)
# ============================================================================

FROM python:3.9-slim

LABEL maintainer="ML Engineer"
LABEL description="Inference container for House Price Prediction"
LABEL version="1.0"

WORKDIR /app

# ----------------------------------------------------------------------------
# SET PYTHON PATH
# ----------------------------------------------------------------------------
# ENV: Sets environment variable (available at runtime)
# PYTHONPATH: Tells Python where to look for modules
# /app: Our working directory, so Python can find 'src' and 'inference' modules
# ----------------------------------------------------------------------------
ENV PYTHONPATH=/app

# ----------------------------------------------------------------------------
# INSTALL SYSTEM DEPENDENCIES (minimal)
# ----------------------------------------------------------------------------
# Inference needs fewer system packages
# - No build tools (pre-compiled packages)
# - Only runtime libraries
# ----------------------------------------------------------------------------
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        libgfortran5 && \
    rm -rf /var/lib/apt/lists/*

# ----------------------------------------------------------------------------
# INSTALL ONLY INFERENCE DEPENDENCIES
# ----------------------------------------------------------------------------
# Create minimal requirements for inference
# Only what's needed for predictions:
# - scikit-learn: Model loading and prediction
# - pandas: Data manipulation
# - numpy: Numerical operations
# - joblib: Model serialization
#
# Excluded (not needed for inference):
# - jupyter: Only for EDA
# - matplotlib: Only for visualization
# - seaborn: Only for visualization
# - kaggle: Only for data download
#
# Why separate requirements?
# - Smaller image (~500MB vs ~2GB)
# - Faster container startup
# - Less attack surface (fewer dependencies = fewer vulnerabilities)
# ----------------------------------------------------------------------------
RUN pip install --no-cache-dir \
    scikit-learn==1.3.2 \
    pandas==2.1.3 \
    numpy==1.24.3 \
    joblib==1.3.2

# ----------------------------------------------------------------------------
# COPY ONLY INFERENCE CODE
# ----------------------------------------------------------------------------
# Copy inference module and necessary source modules
# - inference/: Inference code
# - src/preprocess.py: Needed for preprocessing (same as training)
# - src/data_loader.py: May be needed for data loading
#
# Why not copy all of src/?
# - Inference doesn't need train.py, evaluate.py
# - Smaller image
# - Clearer intent (only inference code)
# ----------------------------------------------------------------------------
COPY inference/ /app/inference/
COPY src/preprocess.py /app/src/preprocess.py
COPY src/data_loader.py /app/src/data_loader.py
COPY src/__init__.py /app/src/__init__.py

# ----------------------------------------------------------------------------
# CREATE MODEL DIRECTORY
# ----------------------------------------------------------------------------
# Models will be mounted as volume at runtime
# This ensures directory exists if volume mount fails
# ----------------------------------------------------------------------------
RUN mkdir -p /app/models

# ----------------------------------------------------------------------------
# DEFAULT COMMAND
# ----------------------------------------------------------------------------
# Default: Run prediction script
# Can be overridden for different use cases:
# - docker run image python inference/predict.py --input data.csv
# - docker run image python -c "from inference.predict import HousePricePredictor; ..."
# ----------------------------------------------------------------------------
CMD ["python", "inference/predict.py", "--help"]

# ----------------------------------------------------------------------------
# INFERENCE CONTAINER CHARACTERISTICS
# ----------------------------------------------------------------------------
# ✅ Lightweight: ~500MB (vs ~2GB for training)
# ✅ Fast startup: Fewer dependencies to load
# ✅ Production-ready: Minimal dependencies
# ✅ Secure: Smaller attack surface
#
# Use Cases:
# - API endpoints (Flask/FastAPI)
# - Batch prediction jobs
# - Real-time inference services
# ----------------------------------------------------------------------------
